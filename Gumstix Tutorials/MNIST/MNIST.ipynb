{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline\n",
    "Your first project will be to build, train, and evaluate a model that identifies images of handwritten digits 0-9 from the MNIST dataset. To do this, we’ll explicitly define a deep neural network (i.e. a neural network with 2 or more hidden layers), including everything from its structure to its activation functions to its optimizer algorithm. Running on top of cuDNN and TensorRT-accelerated hardware, veterans of this dataset will see a noticeable improvement in training time. In outlining this model so concretely, it will be easier to tweak individual parameters or the structure of the network as a whole, allowing for you to explore how each parameter affects model performance. \n",
    "\n",
    "## The Data\n",
    "MNIST contains 55,000 training images and 10,000 testing images. Each image is a 28x28, monochromatic image of a handwritten digit from 0 to 9 and look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/seven.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the dataset into your python project, add the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# one_hot = specifies that only one output node will be \"hot\" or 1, all others will be 0.\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This partitions the data into a training set and a testing set - data to train on and data to evaluate our model’s performance on. A label complements each image, denoting what the digit is shown in the image. We will use this to compare our prediction for each image with its label and develop the network based on any discrepancies. \n",
    "We also need to define how many possible outputs we can have, called “classes.” In this case, since the only option for an answers is a number from 0 to 9, we have 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we have to do is store the image-label pairs, like an (x,y) data point, where the ‘x’ is a one-dimensional array of size 784 (28^2 = 784) containing the pixel values of the image and ‘y’ is the label for that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, 784])  # Data\n",
    "y = tf.placeholder('float')  # Labels of that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to define how many images to send through the network per each training iteration - our “batch size”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This controls how many images will pass through the network before propagating backwards and updating weights. Thus, smaller batch sizes demand longer training times but yield more accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network\n",
    "Now to build out the structure of the network, which will look something like this:\n",
    "\n",
    "<img src=\"./images/network1.png\">\n",
    "\n",
    "We will be making a Feed Forward neural network because data will pass through from input to output. Our network, however, will have 784 input neurons, 10 output neurons, and much more than 9 neurons in each hidden layer. Here’s how the data will flow:\n",
    "\n",
    "<img src=\"./images/network2.png\">\n",
    "\n",
    "The value at each neuron will follow this formula: \n",
    "\n",
    "<img src=\"./images/formula.png\"> \n",
    "\n",
    "Where\n",
    "\tW = the weight coming into that neuron\n",
    "\tx = the input data value (from the previous layer)\n",
    "\tb = the bias (helps if all the data is 0, in which no neurons would otherwise fire)\n",
    "The weights, multiplied by the input data, are summed together. This value is then fed into an “activation function,” but we’ll get to that later. Let’s start by specifying how many neurons (nodes) will be in each hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_hl1 = 500\n",
    "num_nodes_hl2 = 500\n",
    "num_nodes_hl3 = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define each layer. We’ll start by creating a method to hold our model and within it we’ll outline the connections between each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(data):  # The network model\n",
    "    hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([784, num_nodes_hl1])), 'biases': tf.Variable(tf.random_normal([num_nodes_hl1]))}\n",
    "\n",
    "    hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([num_nodes_hl1, num_nodes_hl2])),'biases': tf.Variable(tf.random_normal([num_nodes_hl2]))}\n",
    "\n",
    "    hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([num_nodes_hl2, num_nodes_hl3])),'biases': tf.Variable(tf.random_normal([num_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([num_nodes_hl3, num_classes])),'biases': tf.Variable(tf.random_normal([num_classes]))}\n",
    "    \n",
    "    # Wx + b\n",
    "    layer_1 = tf.add(tf.matmul(data, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "    output = tf.add(tf.matmul(layer_3, output_layer['weights']), output_layer['biases'])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the neurons in the human brain, each node requires an “activation function” to determine whether the neuron should be activated. More broadly, it imparts nonlinearity onto the model, which crucially allows the network to do more complex tasks than a linear regression model. A good general activation function is ReLU (Rectified Linear Unit), which we use here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Now we need to train our model to classify the digits. In order to improve our network, we need to define a metric by which we can judge our performance per iteration - a “cost function.” To do this, we’re going to use the softmax function, which calculates the probability error in discrete, mutually exclusive classification tasks. Basically, for each of our classes, it will output a value from 0 to 1 denoting the probability of each digit being the one depicted in the input image, with the sum of the probabilities equalling 1. We’re then going to extract the value of our cost by getting the mean of the Tensor returned by the softmax function.\n",
    "\n",
    "Now that we have a cost function defined, we need to use it to improve our model. To do so, we’ll use an “optimizer” to minimize our cost function by updating the weights between layers during backpropagation. I’ve chosen to use the Adam optimization algorithm, but others can be used here as well (Ex. Adagrad, gradient descent, RMSProp). \n",
    "\n",
    "When training the network, we can specify how many “epochs” we want it to train for. The more epochs, in theory the more accurate your model, though your improvements do start to diminish. One epoch is one feed forward pass through the network followed by a backpropagation cycle. \n",
    "\n",
    "To train the network, we have to start a TensorFlow session, iterate through each batch, update our cost function, optimize the weights based on that iteration, and then do this for however many epochs as were defined above. To show how your improvement diminishes as your epoch count increases, a percent improvement counter displays how much better the current epoch did compared to its predecessor:\n",
    "\n",
    "With all of this in mind, within a new method add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(x):    \n",
    "    prediction = nn_model(x)\n",
    "    costVal = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(costVal)\n",
    "    \n",
    "    num_epochs = 10\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        percent_improvement = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            current_loss = 0\n",
    "            num_examples = mnist.train.num_examples\n",
    "            # Num_examples / batch size = how many times to cycle\n",
    "            for _ in range(int(num_examples / batch_size)):\n",
    "                # Partitions out the dataset to use each time\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, cost = sess.run([optimizer, costVal], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                current_loss += cost  # Sum up the cost in this epoch\n",
    "            if epoch != 0:\n",
    "                percent_improvement = (1 - (current_loss / previous_loss)) * 100\n",
    "            previous_loss = current_loss\n",
    "            print('Epoch', epoch + 1, '/', num_epochs, ': Loss:', current_loss, \"Percent Improvement:\", percent_improvement)\n",
    "        \n",
    "                # How close are we to the actual answer?\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        # Cast it to a Tensor of numbers\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        # eval() gets the value of the accuracy Tensor\n",
    "        print('Accuracy:', accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that needs to be done now is to run our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()  # Record start time\n",
    "run_nn(x)  # Then train and test the model\n",
    "# And print the elapsed time in seconds\n",
    "print(\"Total time: \" + \"%.2f\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running this notebook, you should see an accuracy score for your model, as well as the time it took to train and run it. In the future, especially for larger models, a trained model can be saved using tf.saved_model.simple_save and restored with tf.saved_model.loader.load, thereby massively decreasing runtime as evaluation is almost always faster than training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
