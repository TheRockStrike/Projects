{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCOnut - Recognizing Common Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we’ll be utilizing the AeroCore 2 for Jetson’s CSI camera capabilities in conjunction with [Google’s Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) to recognize common objects around you.\n",
    "\n",
    "<img src=\"./objdetect.jpeg\">\n",
    "\n",
    "You will see a live feed with superimposed labels for 80 different classes of objects using a deep convolutional neural network. You’ll gain experience with using a large dataset - [COCO (Common Objects in Context) dataset](http://cocodataset.org/#home) - as well as with the [OpenCV API](https://opencv.org/). Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to get the computer vision library that we’ll be using for this project - OpenCV (Open Source Computer Vision Library). OpenCV is a widely-used, open source computer vision and machine learning software library. Installing it can be a bit tedious, but luckily the folks over at JetsonHacks have a nice script to do it all for you. (NOTE: We’ll be using python 3+ for this project). Running the below cell will install all the dependencies and get everything setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do install things manually (optional):\n",
    "Download their repository [here](https://github.com/jetsonhacks/buildOpenCVTX2) or by cloning it onto the Jetson TX2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jetsonhacks/buildOpenCVTX2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for the TX1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jetsonhacks/buildOpenCVTX1.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd into the folder and make a folder for the build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd buildOpenCVTX* && mkdir opencv_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the build script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo ./buildOpenCV.sh -s opencv_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script will take around an hour and a half to build the library. After that is finished, <font color='blue'>Cython</font>, <font color='blue'>pillow</font>, <font color='blue'>lxml</font>, <font color='blue'>jupyter</font>, <font color='blue'>matplotlib</font>, and Google’s <font color='blue'>Protocol Buffers</font> all need to be installed to get up and running.\n",
    "\n",
    "To install the Protocol Buffers (protoc), download the protoc-x.x.x-linux-aarch_64.zip file from the [releases page](https://github.com/google/protobuf/releases) into this project directory:\n",
    "\n",
    "Unzip it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip *aarch_64.zip -d protoc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install it by simply copying over the binaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mv protoc3/bin/* /usr/local/bin/\n",
    "!sudo mv protoc3/include/* /usr/local/include/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the python dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install Cython\n",
    "!sudo pip3 install pillow\n",
    "!sudo pip3 install lxml\n",
    "!sudo pip3 install jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll need some other libraries to install matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install libfreetype6-dev pkg-config libpng12-dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to install matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to download the [TensorFlow models repository](https://github.com/tensorflow/models). There are a lot of interesting models under the /research/ header, from full resolution image compression with RNNs to text summarization, but we’ll be using the object detection API for this project. Download it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd into the /models/research/ directory and setup protoc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd models/research && protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the dependencies are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Script and Configuring the backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google provides a useful starting script for single images, but we want it to work on the Jetson with a live camera feed. We’ll keep many of the same imports and setup as the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is allow the script to input frames from the CSI camera on the AeroCore. We’ll use OpenCV with a <font color='green'>nvcamerasrc</font> pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "pipeline = \"nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1920, height=(int)1080, format=(string)I420, framerate=(fraction)60/1 ! nvvidconv flip-method=2 ! video/x-raw, format=(string)I420 ! videoconvert ! video/x-raw, format=(string)BGR ! appsink\"\n",
    "cap = cv2.VideoCapture(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can specify various options on your input pipeline such as framerate, resolution, orientation, and color. To change the resolution, simply edit the values for <font color='green'>width=</font> and <font color='green'>height=</font>. To change the framerate, change the <font color='green'>framerate=</font> value to whatever framerate you want. The <font color='green'>flip-method=</font> flag can also be changed to different modes to change camera orientation. This pipeline specifies a 1920x1080 resolution at 60FPS rotated 180°. \n",
    "\n",
    "Now to specify and download the model to use. Pick a model from the list of COCO-trained models in the [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models-coco-models). It’s recommended that you pick one of the mobilenets as these are more efficient CNNs designed for mobile use and thus will be faster on the Jetson. Click on it to download and you’ll see it’s file name (Ex. <font color='red'>ssd_mobilenet_v2_coco_2018_03_29.tar</font>). This is what we’ll use in the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'NET_NAME'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NET_NAME with the file name you just found. When the script runs, it will download or load this pre-trained model. This dataset contains 80 different classes of objects so we’ll add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to download the model and load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll create a label map to map numbers to objects. Think of it like a standard hashtable or hashmap. This is so when our CNN predicts a “41,” we know this means a “TV”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the first project, we need to process the each frame into an array of values to pass into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do the detection. We’ll use the detection graph we just downloaded in our TF session to detect and visualize that detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        while True:\n",
    "            ret, image_np = cap.read()\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            # Each box represents a part of the image where a particular object was detected.\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            # Each score represent how level of confidence for each of the objects.\n",
    "            # Score is shown on the result image, together with the class label.\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name(\n",
    "                'num_detections:0')\n",
    "            # Actual detection.\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8)\n",
    "\n",
    "            cv2.imshow('object detection', cv2.resize(image_np, (800, 600)))\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it on the python side. Now we need to change the backend for matplotlib so it will run on the Jetson. To find where the backend is specified, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo vi `python3 -c \"import matplotlib; print(matplotlib.matplotlib_fname())\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command first creates and writes a python script to print the path to the matplotlibrc file, which contains the backend specification, and then passes that output into a vi command for you to edit it. Now in the editor, find the <span style=\"background-color: #000000\"><font color=\"white\"><b>backend : gtk3agg</span></font></b> line and replace <span style=\"background-color: #000000\"><font color=\"white\"><b>gtk3agg</span></font></b> with <span style=\"background-color: #000000\"><font color=\"white\"><b>qt4agg</span></font></b> (remember: enter EDIT mode by hitting “I” and enter COMMAND mode by hitting ESC. Save by typing “:w” and quit with “:q”). \n",
    "\n",
    "Finally, we need to install the qt4agg backend we want it to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install python3-pyqt4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! Though it seems shorter than the much simpler MNIST example, this is a much higher-level project - much more is going on under the hood, especially with the downloaded model. Run the script (<font color=\"green\">python <.py name> or python3 <.py name></font>). It will take a short while to initialize the pipeline and load the model, but you should see a window pop up showing a stream of the CSI camera. It won’t be buttery smooth, but keep in mind that the framerate set in the pipeline is for input into the model. The calculations in between the input and the output use a lot of system resources, thus lowering the output framerate. Now you can point the camera around the room, even at yourself, and watch as it draws label boxes around various objects!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
